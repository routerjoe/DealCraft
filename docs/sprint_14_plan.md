# Sprint 14: ML Refinement (Forecast v2.1)

**Start:** Wednesday, October 29, 2025 EDT  
**Target Merge Window:** Monday, November 10, 2025 EDT

## Objectives

- Refine scoring model to v2.1 with audited bonuses
- Implement feature store for model inputs
- Create offline evaluation framework
- Add guardrails and thresholds
- Document model improvements
- Maintain backward compatibility with v2.0

## Scope (In)

- Scoring Model v2.1 enhancements
- Feature store implementation
- Offline evaluation plan
- Guardrails for score bounds
- Model versioning
- Performance benchmarking
- Documentation of v2.0 â†’ v2.1 changes
- Contract validation tests with TODO markers

## Scope (Out)

- Real-time model retraining
- A/B testing framework
- Model serving infrastructure (separate service)
- Advanced feature engineering (PCA, embeddings)
- Online learning capabilities
- Model explainability UI (Obsidian-based only)
- External ML platform integration (MLflow, Weights & Biases)

## Interfaces & Contracts

### Scoring Model v2.1 Constants

**Location:** `mcp/core/scoring.py`

```python
# Version identifier
SCORING_MODEL_VERSION = "multi_factor_v2.1_audited"

# Audited bonuses (validated against historical data)
REGION_BONUS_AUDITED = {
    "East": 2.5,      # Increased from 2.0 (higher win rate)
    "West": 2.0,      # Maintained
    "Central": 1.5,   # Decreased from 2.0 (lower conversion)
}

CUSTOMER_ORG_BONUS_AUDITED = {
    "DEPARTMENT-ALPHA": 4.0,       # Increased from 3.0 (strategic account)
    "Civilian": 3.0,  # Maintained
    "Default": 2.0,   # For known but not strategic
}

CV_RECOMMENDATION_BONUS_AUDITED = {
    "single": 5.0,    # One CV recommended
    "multiple": 7.0,  # 2+ CVs recommended
}

# Guardrails
MIN_SCORE = 0.0
MAX_SCORE = 100.0
MIN_WIN_PROB = 0.0
MAX_WIN_PROB = 1.0
```

### Feature Store Schema

```python
# Features tracked for each opportunity
FEATURE_SCHEMA = {
    "opportunity_id": "string",
    "oem_alignment": "float",
    "partner_fit": "float",
    "vehicle_score": "float",
    "govly_relevance": "float",
    "amount_score": "float",
    "region_bonus": "float",
    "org_bonus": "float",
    "cv_bonus": "float",
    "stage_probability": "float",
    "time_decay": "float",
    "final_score": "float",
    "win_probability": "float",
    "scored_at": "timestamp",
    "model_version": "string",
}
```

### Offline Evaluation Metrics

- **Accuracy:** % of predictions within Â±10% of actual outcome
- **Precision:** True positives / (True positives + False positives)
- **Recall:** True positives / (True positives + False negatives)
- **F1 Score:** Harmonic mean of precision and recall
- **AUC-ROC:** Area under receiver operating characteristic curve
- **Calibration:** Alignment of predicted vs actual probabilities

## Deliverables

### 1. Documentation
- ðŸ†• `/docs/sprint_plan.md` - This sprint plan
- ðŸ†• `/docs/guides/forecast_model_v2_1.md` - Model v2.1 guide
  - Audited bonus values and rationale
  - Feature store design
  - Offline evaluation methodology
  - Guardrails documentation
  - Migration from v2.0

### 2. Tests
- ðŸ†• `tests/test_scoring_v2_1_contract.py` - Contract validation
  - Assert new constants present
  - Validate guardrails enforced
  - Check model version updated
  - TODO markers for algorithm implementation

### 3. Code (Stubs)
- Placeholder constants in scoring.py
- Version identifier updated
- TODO comments for v2.1 implementation

## Success Criteria / DEPARTMENT-ALPHA

- [ ] Sprint plan created (docs/sprint_plan.md)
- [ ] Model v2.1 guide complete (docs/guides/forecast_model_v2_1.md)
- [ ] Contract tests created (tests/test_scoring_v2_1_contract.py)
- [ ] All tests passing (with TODO markers)
- [ ] Constants defined (even if not active)
- [ ] Guardrails documented
- [ ] Evaluation plan documented
- [ ] Code committed with message "docs(sprint14): add sprint plan + stubs"
- [ ] Branch pushed to origin
- [ ] Draft PR opened with labels: sprint, 14, seed, draft

## Risks & Mitigations

### Risk: Bonus Over-Optimization
**Impact:** High - Inflated scores reduce credibility  
**Probability:** Medium  
**Mitigation:**
- Audit bonuses against historical data
- Set maximum total bonus (e.g., 15%)
- Require validation before activation
- A/B test in shadow mode
- Monitor score distribution

### Risk: Feature Store Complexity
**Impact:** Medium - Adds system dependencies  
**Probability:** Low  
**Mitigation:**
- Start with simple JSON storage
- Document schema clearly
- Provide migration path
- Keep feature extraction simple
- Version feature schema

### Risk: Evaluation Data Quality
**Impact:** High - Poor evaluation = bad decisions  
**Probability:** Medium  
**Mitigation:**
- Validate historical data quality
- Clean outliers and anomalies
- Document data sources
- Include confidence intervals
- Regular evaluation refresh

## Validation Steps

### 1. Contract Tests

```bash
# Run contract validation tests
pytest tests/test_scoring_v2_1_contract.py -v

# Should assert:
# - New constants defined
# - Guardrails present
# - Model version updated
```

### 2. Documentation Review

```bash
# Verify documentation
cat docs/sprint_plan.md
cat docs/guides/forecast_model_v2_1.md
```

### 3. Constant Verification

```bash
# Check constants defined
grep "AUDITED" mcp/core/scoring.py
grep "v2.1" mcp/core/scoring.py
```

## Checklist

- [x] Design reviewed
- [ ] Sprint plan created (docs/sprint_plan.md)
- [ ] Model v2.1 guide created (docs/guides/forecast_model_v2_1.md)
- [ ] Contract tests created (tests/test_scoring_v2_1_contract.py)
- [ ] Constants stubbed in scoring.py (with TODO)
- [ ] All tests passing
- [ ] Documentation complete and reviewed
- [ ] Code committed with message "docs(sprint14): add sprint plan + stubs"
- [ ] Branch pushed to origin
- [ ] Draft PR opened with labels: sprint, 14, seed, draft
- [ ] Ready for development phase

## Notes

- v2.0 already includes region/org/CV bonuses (Phase 9)
- v2.1 will audit and refine these bonuses based on historical performance
- Feature store enables offline evaluation and model monitoring
- Guardrails prevent score drift and ensure reasonable bounds
- Backward compatible - v2.0 scores remain valid

---

**Next Steps After Sprint:**
1. Collect historical win/loss data for evaluation
2. Audit bonus values against actual outcomes
3. Implement feature store persistence
4. Build offline evaluation pipeline
5. Create model monitoring dashboard